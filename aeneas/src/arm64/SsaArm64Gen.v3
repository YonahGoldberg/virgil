// Arm64 instructions are ints that look like <width><mode arg><mode><code>
// where each part of the instruction is a byte

// codes
def MASK_CODE = 0xff;
def I_ADD = 0x00;
def I_MOV = 0x01;
def I_MOVK = 0x02;

// addressing modes
def MASK_AM = 0xff00;
def SHIFT_AM: byte = 8;
def AM_NONE = 0x00;
def AM_IMM = 0x01;
def AM_SHIFTED_IMM = 0x02;
def AM_EXTENDED_REG_LSL = 0x03;
def AM_SHIFTED_REG_LSL = 0x04;
def AM_REG = 0x05;

// addressing mode args
def MASK_AM_ARG = 0xff0000;
def SHIFT_AM_ARG: byte = 16;
def ARG_NONE = 0x00;

// widths (the width of the type the instruction operates on)
def MASK_WIDTH = 0xff000000;
def SHIFT_WIDTH: byte = 24;
def V_32 = 0x00;
def V_64 = 0x01;

// XXX: Add to utils?
type Maybe<T> {
	case None;
	case Some(t: T);
}

// The possible widths of immediates used in Arm64 instructions
type ImmWidth {
	case Imm12;
	case Imm16;
	case Imm19;
}

// Constructs an opcode from the 4 parts
def makeOpcode(code: int, am: int, amArg: int, width: int) -> int {
	return code | (am << SHIFT_AM) | (amArg << SHIFT_AM_ARG) | (width << SHIFT_WIDTH);
}

// Extracts the code section of the opcode
def getCode(opcode: int) -> int {
	return opcode & MASK_CODE;
}

// Inserts the code section of the opcode
def putOpcode(opcode: int, code: int) -> int {
	return opcode | code;
}

// Extracts the addressing mode argument section of the opcode
def getAmArg(opcode: int) -> int {
	return (opcode & MASK_AM_ARG) >> SHIFT_AM_ARG;
}

// Inserts the addressing mode argument section of the opcode
def putAmArg(opcode: int, arg: int) -> int {
	return (arg << SHIFT_AM_ARG) | opcode;
}

// Extracts the addressing mode section of the opcode
def getAm(opcode: int) -> int {
	return (opcode & MASK_AM) >> SHIFT_AM;
}

// Inserts the addressing mode section of the opcode
def putAm(opcode: int, am: int) -> int {
	return (am << SHIFT_AM) | opcode;
}

// Inserts the width section of the opcode
def getWidth(opcode: int) -> int {
	return (opcode & MASK_WIDTH) >> SHIFT_WIDTH;
}

// Extracts the width section of the opcode
def putWidth(opcode: int, width: int) -> int {
	return (width << SHIFT_WIDTH) | opcode;
}

// Inserts the width section of the opcode according to the register class
// of the operands
def putRegClassWidth(opcode: int, rc: RegClass) -> int {
	match (rc) {
		I32, F32 => return (V_32 << SHIFT_WIDTH) | opcode;
		_ => return (V_64 << SHIFT_WIDTH) | opcode;
	}
}

def Regs: Arm64RegSet;
def Conds: Arm64Conds; // TODO

// Code generation for the Arm64 backend
class SsaArm64Gen extends SsaMachGen {
	def asm: Arm64MacroAssembler; // TODO
	def m = SsaInstrMatcher.new();
	def dwarf: Dwarf; // What is this?

	new(context: SsaContext, mach: MachProgram, asm, w: MachDataWriter, dwarf)
	super(context, mach, Arm64RegSet.SET, w) {}

	// Overidden Architecture Specific Routines
	def visitApply(block: SsaBlock, i: SsaApplyOp) {
		match (i.op.opcode) {
			IntAdd => {
				emitIntBinop(I_ADD, i);
			}
			_ => context.fail("not implemented"); // TODO
		}
	}

	def visitThrow(block: SsaBlock, i: SsaThrow) { context.fail("not implemented"); }
	def visitIf(block: SsaBlock, i: SsaIf) { context.fail("not implemented"); }
	def visitSwitch(block: SsaBlock, i: SsaSwitch) { context.fail("not implemented"); }
	def visitGoto(block: SsaBlock, target: SsaGoto) { context.fail("not implemented"); }

	// Override Code Generation
	def assemble(opcode: int, x: Array<Operand>) {
		context.fail("not implemented");
	}

	// Regalloc callbacks to add moves
	def genSaveLocal(reg: int, v: VReg) { context.fail("not implemented"); }
	def genRestoreLocal(v: VReg, reg: int) { context.fail("not implemented"); }
	def genMoveLocLoc(src: (VReg, int), dst: (VReg, int), regClass: RegClass) { context.fail("not implemented"); }

	// Register allocation callback to prepend a move
	def genMoveValLoc(src: VReg, dst: (VReg, int), regClass: RegClass) {
		def operand = getImm32Operand(src.ssa, ImmWidth.Imm16);
		if (regSet.isStack(dst.1)) {
			match (operand) {
				Some(immOp) => {
					// Move constant directly to stack
					emit2(putRegClassWidth(I_MOV, regClass), dfnv(dst), op(immOp));
				}
				None => {
					// Too large constant or mem loc, either way need scratch
					genMoveValLoc(src, (null, Regs.SCRATCH_GPR), regClass);
					genMoveLocLoc((null, Regs.SCRATCH_GPR), dst, regClass);
				}
			}
			return;
		}
		// TODO dst can be an xmm
		match (operand) {
			Some(immOp) => {
				// Move constant directly to reg
				emit2(putRegClassWidth(I_MOV, regClass), dfnv(dst), op(immOp));
			}
			None => {
				// Constant too large
				def val = SsaConst.!(src.ssa).val;
				match (val) {
					x: Box<int> => {
						if (x.val == i16.view(x.val)) {
							// Arm64 mov supports 16-bit immediates
							emit2(putWidth(I_MOV, V_32), dfnv(null, Regs.SCRATCH_GPR), useImm(val));
						} else {
							// mov followed by movk to insert imm in reg
							// move first 16-bits to reg
							emit2(putWidth(I_MOV, V_32), dfnv(null, Regs.SCRATCH_GPR), useImm(Int.box(x.val & 0xffff)));
							// Move second 16-bits to reg
							def opcode = makeOpcode(I_MOVK, AM_SHIFTED_IMM, 16, V_32);
							emit2(opcode, dfnv(null, Regs.SCRATCH_GPR), useImm(Int.box((x.val & 0xffff0000) >> 16)));
						}
					}
					_ => context.fail("not implemented"); // TODO
				}
			}
		}
	}

	// Helper functions
	// Emit code for an integer binop
	def emitIntBinop(code: int, i: SsaApplyOp) {
		def width = getIntOpWidth(i);
		emitSimpleBinop(code, width, i);
	}

	// Emit code for a simple binop (add, sub, mul, etc...)
	def emitSimpleBinop(code: int, width: int, i: SsaApplyOp) {
		// XXX: select better left operand using liveness
		m.intbinop(i);
		def opcode = makeOpcode(code, AM_REG, ARG_NONE, width);
		emit3(opcode, dfnReg(i), use(m.x), use(m.y));
	}

	// Returns the width for an integer operation
	def getIntOpWidth(i: SsaApplyOp) -> int {
		// XXX: factor this out and clean it up
		def t = i.op.typeArgs[0];
		var width: int;

		if (IntType.?(t)) width = IntType.!(t).width;
		else if (t.typeCon.kind == V3Kind.ENUM) width = V3.getVariantTagType(t).width;
		else if (t.typeCon.kind == V3Kind.ENUM_SET) width = V3.getEnumSetType(t).width;
		else width = 64;

		return if(width > 32, V_64, V_32);
	}
}