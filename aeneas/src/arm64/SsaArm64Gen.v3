// Copyright 2024 Virgil Authors. All rights reserved.
// See LICENSE for details of Apache 2.0 license.

// Arm64 instructions are ints that look like <arg2><arg1><am><code>
// where each part of the instruction is a byte

// masks
def MASK_CODE = 0xff;
def MASK_AM = 0xff00;
def MASK_ARG1 = 0xff0000;
def MASK_ARG2 = 0xff000000;

// shifts
def SHIFT_CODE: byte = 0;
def SHIFT_AM: byte = 8;
def SHIFT_ARG1: byte = 16;
def SHIFT_ARG2: byte = 24;

// codes
def I_ADDD: byte = 0x01;	def I_ADDQ: byte = 0x11;
def I_SUBD: byte = 0x02;	def I_SUBQ: byte = 0x12;

def I_QD_DIFF: byte = I_ADDQ - I_ADDD;

// addressing modes
def AM_NONE: byte = 0x00;
def AM_R_R_I_I: byte = 0x01;
def AM_R_R_R_SH_I: byte = 0x02;
def AM_R_R_R_EX_I: byte = 0x03;

// arguments
def ARG_NONE: byte = 0x00;
def ARG_SH_NONE: byte = 0x01;
def ARG_SH_LSL: byte = 0x02;
def ARG_SH_LSR: byte = 0x03;
def ARG_SH_ASR: byte = 0x04;
def ARG_DATA_EX_UXTB: byte = 0x05;
def ARG_DATA_EX_UXTH: byte = 0x06;
def ARG_DATA_EX_UXTX: byte = 0x07;
def ARG_DATA_EX_UXTW: byte = 0x08;
def ARG_DATA_EX_SXTB: byte = 0x09;
def ARG_DATA_EX_SXTH: byte = 0x0A;
def ARG_DATA_EX_SXTW: byte = 0x0B;
def ARG_DATA_EX_SXTX: byte = 0x0C;
def ARG_MEM_EX_UXTW: byte = 0x0D;
def ARG_MEM_EX_LSL: byte = 0x0E;
def ARG_MEM_EX_SXTW: byte = 0x0F;
def ARG_MEM_EX_SXTX: byte = 0x10;

def makeOpcode(code: byte, am: byte, arg1: byte, arg2: byte) -> int {
	return (int.view(code) << SHIFT_CODE) | (int.view(am) << SHIFT_AM)
		 | (int.view(arg1) << SHIFT_ARG1) | (int.view(arg2) << SHIFT_ARG2);
}

def getCode(opcode: int) -> byte {
	return byte.view((opcode & MASK_CODE) >> SHIFT_CODE);
}

def getAM(opcode: int) -> byte {
	return byte.view((opcode & MASK_AM) >> SHIFT_AM);
}

def getArg1(opcode: int) -> byte {
	return byte.view((opcode & MASK_ARG1) >> SHIFT_ARG1);
}

def getArg2(opcode: int) -> byte {
	return byte.view((opcode & MASK_ARG2) >> SHIFT_ARG2);
}

// useful constants
def MAX_IMM16_MOV = 0xffff;
def MIN_IMM16_MOV = 0xffff0000;

def MATCH_NEG = true;

def MRegs: Arm64RegSet;
def Regs: Arm64Regs;
def Conds: Arm64Conds; // TODO

// Code generation for the Arm64 backend
class SsaArm64Gen extends SsaMachGen {
	def asm: Arm64Assembler; // TODO
	def m = SsaInstrMatcher.new();
	def dwarf: Dwarf; // What is this?

	new(context: SsaContext, mach: MachProgram, asm, w: MachDataWriter, dwarf)
	super(context, mach, Arm64RegSet.SET, w) {}

	// Overidden Architecture Specific Routines
	def visitApply(block: SsaBlock, i: SsaApplyOp) {
		match (i.op.opcode) {
			IntAdd => {
				emitIntBinop(I_ADDD, i);
			}
			IntSub => {
				m.intbinop(i);
				if (MATCH_NEG && m.xconst && m.xint == 0) return emit1(I_NEGQ | (AM_OP << AM_SHIFT), ovwReg(i, m.y));
				emitIntBinop(I_SUBD, i);
			}
			_ => unimplemented(); // TODO
		}
	}

	def visitThrow(block: SsaBlock, i: SsaThrow) { unimplemented(); }
	def visitIf(block: SsaBlock, i: SsaIf) { unimplemented(); }
	def visitSwitch(block: SsaBlock, i: SsaSwitch) { unimplemented(); }
	def visitGoto(block: SsaBlock, target: SsaGoto) { unimplemented(); }

	// Regalloc callbacks to add moves
	def genSaveLocal(reg: int, v: VReg) { unimplemented(); }
	def genRestoreLocal(v: VReg, reg: int) { unimplemented(); }

	def genMoveLocLoc(src: (VReg, int), dst: (VReg, int), regClass: RegClass) {
		unimplemented();
	}

	// Register allocation callback to prepend a move
	def genMoveValLoc(src: VReg, dst: (VReg, int), regClass: RegClass) {
		unimplemented();
	}

	def assemble(opcode: int, a: Array<Operand>) {
		if (opcode < 0) {
			match (opcode) {
				ArchInstrs.ARCH_ENTRY => {
					var adjust = frameAdjust();
					// allocate frame
					if (adjust > 0) asm.subq_r_r_i_i(Regs.SP, Regs.SP, u12.view(adjust), 0);
				}
				ArchInstrs.ARCH_BLOCK => return; // TODO
				ArchInstrs.ARCH_RET => {
					var adjust = frameAdjust();
					// deallocate frame
					if (adjust > 0) asm.addq_r_r_i_i(Regs.SP, Regs.SP, u12.view(adjust), 0);
					asm.ret();
					return;
				}
				ArchInstrs.ARCH_BLOCK_END => return; //TODO
				ArchInstrs.ARCH_END => return;
				_ => unimplemented();
			}
			return;
		}

		def am = getAM(opcode);
		match (getAM(opcode)) {
			AM_R_R_I_I => {
				def imm = u12.view(toB32(toImm(a[2])));
				def	lsl12 = u1.view(getArg1(opcode));
				assemble_r_r_i_i(toGpr(a[0]), toGpr(a[1]), imm, lsl12, opcode);
			}
			AM_R_R_R_SH_I => {
				def sh = toRegShift(getArg1(opcode));
				def imm = u6.view(getArg2(opcode));
				assemble_r_r_r_sh_i(toGpr(a[0]), toGpr(a[1]), toGpr(a[2]), sh, imm, opcode);
			}
			AM_R_R_R_EX_I => {
				def ex = toDataRegExtend(getArg1(opcode));
				def imm = u3.view(getArg2(opcode));
				assemble_r_r_r_ex_i(toGpr(a[0]), toGpr(a[1]), toGpr(a[2]), ex, imm, opcode);
			}
			_ => return context.fail1("unknown addressing mode %d", am);
		}
	}

	// Helper functions

	def assemble_r_r_i_i(rd: Arm64Gpr, rn: Arm64Gpr, imm: u12, lsl12: u1, opcode: int) {
		match (getCode(opcode)) {
			I_ADDD => asm.addd_r_r_i_i(rd, rn, imm, lsl12);
			I_SUBD => asm.subd_r_r_i_i(rd, rn, imm, lsl12);

			I_SUBQ => asm.subq_r_r_i_i(rd, rn, imm, lsl12);
			I_ADDQ => asm.addq_r_r_i_i(rd, rn, imm, lsl12);
			_ => invalidOpcode(opcode);
		}
	}
	def assemble_r_r_r_sh_i(rd: Arm64Gpr, rn: Arm64Gpr, rm: Arm64Gpr, sh: RegShift, imm: u6, opcode: int) {
		match (getCode(opcode)) {
			I_ADDD => asm.addd_r_r_r_sh_i(rd, rn, rm, sh, u5.view(imm));
			I_SUBD => asm.subd_r_r_r_sh_i(rd, rn, rm, sh, u5.view(imm));

			I_ADDQ => asm.addq_r_r_r_sh_i(rd, rn, rm, sh, imm);
			I_SUBQ => asm.subq_r_r_r_sh_i(rd, rn, rm, sh, imm);
			_ => invalidOpcode(opcode);
		}
	}
	def assemble_r_r_r_ex_i(rd: Arm64Gpr, rn: Arm64Gpr, rm: Arm64Gpr, ex: DataRegExtend, imm: u3, opcode: int) {
		match (getCode(opcode)) {
			I_ADDD => asm.addd_r_r_r_ex_i(rd, rn, rm, ex, imm);
			I_SUBD => asm.addd_r_r_r_ex_i(rd, rn, rm, ex, imm);

			I_ADDQ => asm.subq_r_r_r_ex_i(rd, rn, rm, ex, imm);
			I_SUBQ => asm.addq_r_r_r_ex_i(rd, rn, rm, ex, imm);
			_ => invalidOpcode(opcode);
		}
	}

	def selectWidth(i: SsaApplyOp, code: byte) -> byte {
		return if(intOpWidth(i) > 32, code + I_QD_DIFF, code);
	}

	def intOpWidth(i: SsaApplyOp) -> byte {
		// XXX: factor this out and clean it up
		var t = i.op.typeArgs[0];
		if (IntType.?(t)) return IntType.!(t).width;
		if (t.typeCon.kind == Kind.ENUM) return V3.getVariantTagType(t).width;
		if (t.typeCon.kind == Kind.ENUM_SET) return V3.getEnumSetType(t).width;
		return 64;
	}

	// Emit code for an integer binop
	def emitIntBinop(code: byte, i: SsaApplyOp) {
		def width = intOpWidth(i);
		emitSimpleBinop(selectWidth(i, code), i);
	}

	// Emit code for a simple binop (add, sub, mul, etc...)
	def emitSimpleBinop(code: byte, i: SsaApplyOp) {
		// XXX: select better left operand using liveness
		m.intbinop(i);
		dfnReg(i);
		use(m.x);

		var opcode: int;
		if (tryUseImm32(m.y)) {
			opcode = makeOpcode(code, AM_R_R_I_I, 0, ARG_NONE);
		} else {
			opcode = makeOpcode(code, AM_R_R_R_SH_I, ARG_SH_LSL, 0);
			use(m.y);
		}
		emitN(opcode);
	}

	def tryUseImm32(i: SsaInstr) -> bool {
		if (i == null) { useInt(0); return true; }
		if (SsaConst.?(i)) {
			var val = SsaConst.!(i).val;
			match (val) {
				null => { useImm(val); return true; }
				x: Box<int> => { useImm(val); return true; }
				x: Box<long> => if(x.val == int.view(x.val)) { useInt(int.view(x.val)); return true; }
				x: Addr => { useImm(val); return true; }
				x: Box<bool> => { useInt(if(x.val, 1, 0)); return true; }
				x: ArrayRangeStart => { useImm(val); return true; }
			}
		}
		return false;
	}

	def toLoc(o: Operand) -> int {
		match (o) {
			Overwrite(dst, src, assignment) => return assignment;
			Def(vreg, assignment) => return assignment;
			Use(vreg, assignment) => return assignment;
			_ => return V3.fail("expected operand with assignment");
		}
	}

	def toGpr(o: Operand) -> Arm64Gpr {
		return loc_r(toLoc(o));
	}

	def loc_r(loc: int) -> Arm64Gpr {
		var gpr = MRegs.toGpr(loc);
		if (gpr == null) return V3.fail1("expected GPR, got %s", regSet.identify(loc));
		return gpr;
	}

	def toB32(val: Val) -> int {
		var addr: Addr, b: int;
		match (val) {
			x: Box<int> 		=> b = x.val;
			x: Box<long> 		=> b = int.view(x.val);
			x: Box<bool> 		=> b = if(x.val, 1);
			x: Float32Val 		=> b = int.view(x.bits);
			x: Float64Val 		=> b = int.view(x.bits);
			x: ArrayRangeStart 	=> b = x.start;
			null 			=> b = 0;
			_ => ;
		}
		return b;
	}

	def toRegShift(sh: byte) -> RegShift {
		match (sh) {
			ARG_SH_NONE => return RegShift.NONE;
			ARG_SH_LSL => return RegShift.LSL;
			ARG_SH_LSR => return RegShift.LSR;
			ARG_SH_ASR => return RegShift.ASR;
			_ => {
				context.fail1("unknown reg shift %d", sh);
				return RegShift.NONE;
			}
		}
	}
	def toDataRegExtend(ex: byte) -> DataRegExtend {
		match (ex) {
			ARG_DATA_EX_SXTB => return DataRegExtend.SXTB;
			ARG_DATA_EX_SXTH => return DataRegExtend.SXTH;
			ARG_DATA_EX_SXTW => return DataRegExtend.SXTW;
			ARG_DATA_EX_SXTX => return DataRegExtend.SXTX;
			ARG_DATA_EX_UXTB => return DataRegExtend.UXTB;
			ARG_DATA_EX_UXTH => return DataRegExtend.UXTH;
			ARG_DATA_EX_UXTW => return DataRegExtend.UXTW;
			ARG_DATA_EX_UXTX => return DataRegExtend.UXTX;
			_ => {
				context.fail1("unknown data reg extend %d", ex);
				return DataRegExtend.SXTB;
			}
		}
	}
	def toMemRegExtend(ex: byte) -> MemRegExtend {
		match (ex) {
			ARG_MEM_EX_UXTW => return MemRegExtend.UXTW;
			ARG_MEM_EX_LSL => return MemRegExtend.LSL;
			ARG_MEM_EX_SXTW => return MemRegExtend.SXTW;
			ARG_MEM_EX_SXTX => return MemRegExtend.SXTX;
			_ => {
				context.fail1("unknown data mem extend %d", ex);
				return MemRegExtend.LSL;
			}
		}
	}

	def frameAdjust() -> int {
		// assumes return address already pushed
		return frame.size() - mach.code.addressSize;
	}

	def invalidOpcode(opcode: int) {
		context.fail(Strings.format2("invalid opcode am=%x code=%x", getAM(opcode), getCode(opcode)));
	}

	def unimplemented() { context.fail("unimplemented"); }
}